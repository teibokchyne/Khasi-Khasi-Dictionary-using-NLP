{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e9e419f",
   "metadata": {},
   "source": [
    "# Collect vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b48429dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d7e5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 485859 words to outputs/verified_vocabulary/khasi_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "def extract_vocab_from_dir(input_dir, output_path, top_k=None):\n",
    "    word_counter = Counter()\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(input_dir, filename), 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    words = re.findall(r'\\b[\\w\\-]+\\b', line.lower())\n",
    "                    word_counter.update(words)\n",
    "\n",
    "    most_common = word_counter.most_common(top_k)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for word, count in most_common:\n",
    "            f.write(f\"{word}\\t{count}\\n\")\n",
    "\n",
    "    print(f\"✅ Saved {len(most_common)} words to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    extract_vocab_from_dir(\n",
    "        input_dir=\"outputs/full_sentences\",\n",
    "        output_path=\"outputs/collected_vocabulary/khasi_vocab.txt\",\n",
    "        top_k=None  # You can use 5000 or 10000 to limit\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2f74b",
   "metadata": {},
   "source": [
    "# Separate the vocabulary into sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158fe23f",
   "metadata": {},
   "source": [
    "1. remove the counters from khasi_vocab. \n",
    "2. keep only the words\n",
    "3. manually tag the khasi_vocab_without_counter with 1,2,3,4 and so on so that we can use python to automatically detect and populate the files in collected_vocabulary folder\n",
    "   1. valid_words\n",
    "   2. invalid_words\n",
    "   3. proper_nouns\n",
    "   4. unclear_meanings\n",
    "   5. word_substitution\n",
    "   6. alt+0239 ï\n",
    "   7. alt+164 ñ\n",
    "4. ERROR: Due to issues with invisible characters (invisible to the reader but not to the computer) the words could not be extracted properly for replacement and caused issues. \n",
    "5. Now I am waiting for a clean corpus where the invisible characters are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a61beb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_output_folder = os.listdir(\"outputs/collected_vocabulary\")\n",
    "\n",
    "with open(\"outputs/collected_vocabulary/khasi_vocab.txt\",'r') as khasi_vocab, \\\n",
    "open(\"outputs/collected_vocabulary/khasi_vocab_without_counter.txt\",'w') as khasi_vocab_without_counter:\n",
    "    i=0\n",
    "    for line in khasi_vocab:\n",
    "        i+=1\n",
    "        word = line.split()[0]\n",
    "        desired_length = 22\n",
    "        if len(word)<desired_length:\n",
    "            word = word+\"\".join([\" \" for i in range(desired_length-len(word))])\n",
    "        if i>781:\n",
    "            khasi_vocab_without_counter.write(f\"{word}\\t:1\\t:2\\t:3\\t:4\\t:5\\n\")\n",
    "        else:\n",
    "            khasi_vocab_without_counter.write(f\"{word}\\t:1w\\t2\\t:3\\t:4\\t:5\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1341be70",
   "metadata": {},
   "source": [
    "# Uptil row no 3215 of khasi_vocab_without_counter.txt \n",
    "I was using a system of putting the above numbers and a w after the number to indicate the valid, invalid, proper noun, unsure, and replace words.\n",
    "I will write the code below to segregate the words and to change the original files according to the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8332744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all the sentences from all books into one file\n",
    "input_path = r\"outputs\\full_sentences\"\n",
    "output_path = r\"outputs\\full_text\"\n",
    "\n",
    "files  = os.listdir(input_path)\n",
    "for file in files:\n",
    "    with open(os.path.join(input_path,file),'r',encoding='utf-8') as input_file, \\\n",
    "        open(os.path.join(output_path,\"full_text.txt\"),'a',encoding='utf-8') as output_file:\n",
    "        for line in input_file:\n",
    "            output_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e916c890",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mfull_text\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mfull_text.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m full_text, \\\n\u001b[32m     19\u001b[39m      \u001b[38;5;28mopen\u001b[39m(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mfull_text\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mfull_text2.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m full_text2:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m full_text:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m         replaced_line = \u001b[43mpattern\u001b[49m\u001b[43m.\u001b[49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplace_match\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m         full_text2.write(replaced_line)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mreplace_match\u001b[39m\u001b[34m(match)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreplace_match\u001b[39m(match):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m map_words_to_replace[\u001b[43mmatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Select all the words that need to replaced and replace them in the full_text.txt\n",
    "input_file = r\"outputs\\collected_vocabulary\\khasi_vocab_without_counter.txt\"\n",
    "map_words_to_replace = dict()\n",
    "with open(input_file,'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        tm = line.split(\":\")\n",
    "        if len(tm[-1])>1:\n",
    "            new_word = tm[-1][2:].strip()\n",
    "            to_replace = tm[0].strip()\n",
    "            map_words_to_replace[to_replace]=new_word\n",
    "\n",
    "# Compile regex for exact word replacement\n",
    "pattern = re.compile(r'\\b(' + '|'.join(re.escape(k) for k in map_words_to_replace) + r')\\b')\n",
    "\n",
    "def replace_match(match):\n",
    "    return map_words_to_replace[match.group(0)]\n",
    "\n",
    "with open(r\"outputs\\full_text\\full_text.txt\", 'r', encoding='utf-8') as full_text, \\\n",
    "     open(r\"outputs\\full_text\\full_text2.txt\", 'w', encoding='utf-8') as full_text2:\n",
    "    for line in full_text:\n",
    "        replaced_line = pattern.sub(replace_match, line)\n",
    "        full_text2.write(replaced_line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "khasi-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
