{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bafcaaa",
   "metadata": {},
   "source": [
    "1. I was about to clean the collected corpus from internetarchive. Then I looked at what others have done and decided to scrape from Khasi newspapers for a larger corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5960f923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://mawphor.com/page/1/\n",
      "Scraping: https://mawphor.com/page/2/\n",
      "Scraping: https://mawphor.com/page/3/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_mawphor_articles(base_url, num_pages=5):\n",
    "    articles = []\n",
    "\n",
    "    for page in range(1, num_pages + 1):\n",
    "        url = f\"{base_url}/page/{page}/\"\n",
    "        print(f\"Scraping: {url}\")\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch page {page}\")\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.find_all('a', href=True)\n",
    "\n",
    "        # Filter article URLs (customize this per site)\n",
    "        article_links = [a['href'] for a in links if '/news/' in a['href']]\n",
    "        article_links = list(set(article_links))  # Remove duplicates\n",
    "\n",
    "        for article_url in article_links:\n",
    "            try:\n",
    "                article_response = requests.get(article_url)\n",
    "                article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "                content_div = article_soup.find('div', class_='post-content')  # Adjust class name\n",
    "                if content_div:\n",
    "                    article_text = content_div.get_text(separator=' ', strip=True)\n",
    "                    articles.append(article_text)\n",
    "                time.sleep(1)  # be polite\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping {article_url}: {e}\")\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Example use\n",
    "khasi_articles = scrape_mawphor_articles(\"https://mawphor.com\", num_pages=3)\n",
    "\n",
    "# Save to file\n",
    "with open(\"downloads_newspapers/khasi_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for article in khasi_articles:\n",
    "        f.write(article + \"\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "khasi-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
